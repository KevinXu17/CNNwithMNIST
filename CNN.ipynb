{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "4372ab32-6a02-4b04-98d9-b63363aa8a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "d77c365f-b9e7-47f3-96f0-de26c7bc4589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST: mean std => normalization\n",
    "mean_gray = 0.1307\n",
    "stddev_gray = 0.3081\n",
    "\n",
    "# Transformer    img => tensors\n",
    "transforms_ori = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((mean_gray,), (stddev_gray,))])\n",
    "\n",
    "transforms_photo = transforms.Compose([transforms.Resize((28,28)),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize((mean_gray,), (stddev_gray,))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "05224a96-f677-4652-830d-2b2753329442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load our dataset\n",
    "train_dataset = datasets.MNIST(root = './data', \n",
    "                            train = True, \n",
    "                            transform = transforms_ori,\n",
    "                            download = True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root = './data', \n",
    "                            train = False, \n",
    "                            transform = transforms_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "0394ca61-cce8-411c-bfac-b7df337983ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7feeb77a2c40>"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## img\n",
    "random_image = train_dataset[20][0].numpy() * stddev_gray + mean_gray\n",
    "plt.imshow(random_image.reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "01e0fe37-26dd-40df-a5d5-faf27e7f28fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## label\n",
    "train_dataset[20][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "29be90c8-7b53-4729-8d25-0614be587640",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "epochs = 10\n",
    "train_load = torch.utils.data.DataLoader(dataset = train_dataset, \n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle = True)\n",
    "\n",
    "test_load = torch.utils.data.DataLoader(dataset = test_dataset, \n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "457e4291-6553-4d31-8687-4ec838156e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60000 images in the training set\n",
      "There are 10000 images in the test set\n",
      "There are 600 batches in the train loader\n",
      "There are 100 batches in the testloader\n"
     ]
    }
   ],
   "source": [
    "print('There are {} images in the training set'.format(len(train_dataset)))\n",
    "print('There are {} images in the test set'.format(len(test_dataset)))\n",
    "print('There are {} batches in the train loader'.format(len(train_load)))\n",
    "print('There are {} batches in the testloader'.format(len(test_load)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "c8341b16-c3a1-4b0d-817a-8aa8ea9a1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN,self).__init__()\n",
    "        # input size = output size => need to compute by kernel size\n",
    "        #Same Padding = [(filter size - 1) / 2] (Same Padding--> input size = output size)\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=8, kernel_size=3,stride=1, padding=1)\n",
    "        #The output size of each of the 8 feature maps is \n",
    "        #[(input_size - filter_size + 2(padding) / stride) +1] --> [(28-3+2(1)/1)+1] = 28 (padding type is same)\n",
    "        #Batch normalization\n",
    "        self.batchnorm1 = nn.BatchNorm2d(8)\n",
    "        #RELU\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        #After max pooling, the output of each feature map is now 28/2 = 14\n",
    "        self.cnn2 = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        #Output size of each of the 32 feature maps remains 14\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        #After max pooling, the output of each feature map is 14/2 = 7\n",
    "        #Flatten the feature maps. You have 32 feature maps, each of them is of size 7x7 --> 32*7*7 = 1568\n",
    "        self.fc1 = nn.Linear(in_features=1568, out_features=700)\n",
    "        self.droput = nn.Dropout(p=0.5)\n",
    "        self.fc2 = nn.Linear(in_features=700, out_features=10)\n",
    "    def forward(self,x):\n",
    "        out = self.cnn1(x)\n",
    "        out = self.batchnorm1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.maxpool1(out)\n",
    "        # out = self.cnn2(out)\n",
    "        # out = self.batchnorm2(out)\n",
    "        # out = self.relu(out)\n",
    "        # out = self.maxpool2(out)\n",
    "        #Now we have to flatten the output. This is where we apply the feed forward neural network as learned before! \n",
    "        #It will take the shape (batch_size, 1568) = (100, 1568)\n",
    "        out = out.view(-1,1568)\n",
    "        #Then we forward through our fully connected layer \n",
    "        out = self.fc1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.droput(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "559c2439-8ba0-40c7-8210-3a3f2be03620",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "CUDA = torch.cuda.is_available()\n",
    "if CUDA:\n",
    "    model = model.cuda()    \n",
    "loss_fn = nn.CrossEntropyLoss()        \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "67d2dd21-7b3d-43ba-90c4-010a3eb84a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For one iteration, this is what happens:\n",
      "Input Shape: torch.Size([100, 1, 28, 28])\n",
      "Labels Shape: torch.Size([100])\n",
      "Outputs Shape torch.Size([100, 10])\n",
      "Predicted Shape torch.Size([100])\n",
      "Predicted Tensor:\n",
      "tensor([7, 0, 2, 9, 3, 0, 0, 0, 5, 0, 0, 5, 0, 0, 1, 9, 5, 1, 3, 0, 0, 0, 2, 5,\n",
      "        7, 0, 4, 4, 9, 2, 1, 5, 0, 0, 9, 2, 4, 4, 7, 0, 5, 3, 0, 9, 1, 9, 0, 0,\n",
      "        6, 3, 0, 2, 6, 5, 4, 0, 0, 5, 4, 0, 9, 0, 0, 4, 0, 4, 0, 0, 5, 5, 5, 1,\n",
      "        3, 0, 0, 0, 4, 0, 3, 6, 0, 0, 0, 5, 5, 9, 4, 4, 0, 0, 0, 5, 5, 9, 2, 0,\n",
      "        5, 1, 9, 0])\n",
      "Correct Predictions:  tensor(9)\n",
      "Correct Predictions: tensor(9)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iteration = 0\n",
    "correct_nodata = 0\n",
    "correct_data = 0\n",
    "for i,(inputs,labels) in enumerate (train_load):\n",
    "    if iteration==1:\n",
    "        break\n",
    "    inputs = Variable(inputs)\n",
    "    labels = Variable(labels)\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "    print(\"For one iteration, this is what happens:\")\n",
    "    print(\"Input Shape:\",inputs.shape)\n",
    "    print(\"Labels Shape:\",labels.shape)\n",
    "    output = model(inputs)\n",
    "    print(\"Outputs Shape\",output.shape)\n",
    "    _, predicted_nodata = torch.max(output, 1)\n",
    "    print(\"Predicted Shape\",predicted_nodata.shape)\n",
    "    print(\"Predicted Tensor:\")\n",
    "    print(predicted_nodata)\n",
    "    correct_nodata += (predicted_nodata == labels).sum()\n",
    "    print(\"Correct Predictions: \",correct_nodata)\n",
    "    _, predicted_data = torch.max(output.data, 1)\n",
    "    correct_data += (predicted_data == labels.data).sum()\n",
    "    print(\"Correct Predictions:\",correct_data)\n",
    "    \n",
    "\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c6a4a-7292-4d8b-b8d5-7d0d9b824e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.591, Training Accuracy: 84.053, Testing Loss: 0.236, Testing Acc: 93.560\n",
      "Epoch 2/10, Training Loss: 0.243, Training Accuracy: 92.933, Testing Loss: 0.162, Testing Acc: 95.250\n",
      "Epoch 3/10, Training Loss: 0.178, Training Accuracy: 94.845, Testing Loss: 0.123, Testing Acc: 96.350\n",
      "Epoch 4/10, Training Loss: 0.143, Training Accuracy: 95.897, Testing Loss: 0.103, Testing Acc: 97.020\n",
      "Epoch 5/10, Training Loss: 0.119, Training Accuracy: 96.600, Testing Loss: 0.089, Testing Acc: 97.350\n",
      "Epoch 6/10, Training Loss: 0.105, Training Accuracy: 96.962, Testing Loss: 0.080, Testing Acc: 97.630\n"
     ]
    }
   ],
   "source": [
    "#Training the CNN\n",
    "num_epochs = 10\n",
    "\n",
    "#Define the lists to store the results of loss and accuracy\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "#Training\n",
    "for epoch in range(num_epochs): \n",
    "    #Reset these below variables to 0 at the begining of every epoch\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "    iter_loss = 0.0\n",
    "    \n",
    "    model.train()                   # Put the network into training mode\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(train_load):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # If we have GPU, shift the data to GPU\n",
    "        CUDA = torch.cuda.is_available()\n",
    "        if CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()            # Clear off the gradient in (w = w - gradient)\n",
    "        outputs = model(inputs)         \n",
    "        loss = loss_fn(outputs, labels)  \n",
    "        iter_loss += loss.item()       # Accumulate the loss\n",
    "        loss.backward()                 # Backpropagation \n",
    "        optimizer.step()                # Update the weights\n",
    "        \n",
    "        # Record the correct predictions for training data \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum()\n",
    "        iterations += 1\n",
    "    \n",
    "    # Record the training loss\n",
    "    train_loss.append(iter_loss/iterations)\n",
    "    # Record the training accuracy\n",
    "    train_accuracy.append((100 * correct / len(train_dataset)))\n",
    "   \n",
    "    #Testing\n",
    "    loss = 0.0\n",
    "    correct = 0\n",
    "    iterations = 0\n",
    "\n",
    "    model.eval()                    # Put the network into evaluation mode\n",
    "    \n",
    "    for i, (inputs, labels) in enumerate(test_load):\n",
    "        \n",
    "        # Convert torch tensor to Variable\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        CUDA = torch.cuda.is_available()\n",
    "        if CUDA:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "        \n",
    "        outputs = model(inputs)     \n",
    "        los = loss_fn(outputs, labels) # Calculate the loss\n",
    "        loss += los.item()\n",
    "        # Record the correct predictions for training data\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum()\n",
    "        \n",
    "        iterations += 1\n",
    "\n",
    "    # Record the Testing loss\n",
    "    test_loss.append(loss/iterations)\n",
    "    # Record the Testing accuracy\n",
    "    test_accuracy.append((100 * correct / len(test_dataset)))\n",
    "    \n",
    "    print ('Epoch {}/{}, Training Loss: {:.3f}, Training Accuracy: {:.3f}, Testing Loss: {:.3f}, Testing Acc: {:.3f}'\n",
    "           .format(epoch+1, num_epochs, train_loss[-1], train_accuracy[-1], \n",
    "             test_loss[-1], test_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e2167-2460-4831-88de-cff671387288",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the model\n",
    "torch.save(model.state_dict(),'CNN_MNIST.pth')\n",
    "#load the model \n",
    "# model.load_state_dict(torch.load('CNN_MNIST.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f88a675-f15a-4500-bf07-5f3d572c8592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "f = plt.figure(figsize=(10, 10))\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(test_loss, label='Testing Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ecf90f-6e55-46d4-8c00-e44bcd7900ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "f = plt.figure(figsize=(10, 10))\n",
    "plt.plot(train_accuracy, label='Training Accuracy')\n",
    "plt.plot(test_accuracy, label='Testing Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c6660-7137-4016-913d-1624f793cc31",
   "metadata": {},
   "source": [
    "## Test my img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1302a6-c89e-4a51-a26b-b37e9618588c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load sample image\n",
    "file = 'my_6.png'\n",
    "test_image = cv2.imread(file, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Preview sample image\n",
    "plt.imshow(test_image, cmap='gray')\n",
    "\n",
    "# Format Image\n",
    "img_resized = cv2.resize(test_image, (28, 28), interpolation=cv2.INTER_LINEAR)\n",
    "img_resized = cv2.bitwise_not(img_resized)\n",
    "\n",
    "# Preview reformatted image\n",
    "plt.imshow(img_resized, cmap='gray')\n",
    "\n",
    "#my_6 = tf.convert_to_tensor(img_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571a2446-b039-4c0c-9bf2-4084f1abda2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = transforms.ToTensor()(img_resized)\n",
    "images = img.view(1,1,28,28)    \n",
    "if CUDA:\n",
    "    images = images.cuda()\n",
    "    labels = labels.cuda()\n",
    "\n",
    "outputs = model(images)       \n",
    "_, predicted = torch.max(outputs.data, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca32ee4-67b5-4a7e-8a99-dec36e7100a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d7005f-49a8-49af-8ca5-7b14d22168ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59256b40-8983-4b1a-b803-22dd5212a9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
